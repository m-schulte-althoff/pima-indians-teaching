{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fd1fb8",
   "metadata": {},
   "source": [
    "# Coding Block 2 - Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39695e0",
   "metadata": {},
   "source": [
    "### Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0c94e",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-20T20:03:00.442022Z",
     "iopub.status.busy": "2021-09-20T20:03:00.441258Z",
     "iopub.status.idle": "2021-09-20T20:03:01.852024Z",
     "shell.execute_reply": "2021-09-20T20:03:01.851352Z",
     "shell.execute_reply.started": "2021-09-20T19:54:12.026788Z"
    },
    "papermill": {
     "duration": 1.435807,
     "end_time": "2021-09-20T20:03:01.852223",
     "exception": false,
     "start_time": "2021-09-20T20:03:00.416416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "'''\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaae9e8",
   "metadata": {},
   "source": [
    "### Read the dataset \n",
    "You can also compare processed and non-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1775d95",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-09-20T20:03:01.895944Z",
     "iopub.status.busy": "2021-09-20T20:03:01.895175Z",
     "iopub.status.idle": "2021-09-20T20:03:05.794695Z",
     "shell.execute_reply": "2021-09-20T20:03:05.795244Z",
     "shell.execute_reply.started": "2021-09-20T19:54:12.050571Z"
    },
    "papermill": {
     "duration": 3.924865,
     "end_time": "2021-09-20T20:03:05.795452",
     "exception": false,
     "start_time": "2021-09-20T20:03:01.870587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diab=pd.read_csv('diabetes.csv')\n",
    "diab_cleaned=pd.read_csv('diabetes_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5937d7",
   "metadata": {},
   "source": [
    "### Copy the code from your last successful classifiers (RF, XGBoost, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878256be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = {\n",
    "    \"Original Dataset\": diab,\n",
    "    \"Cleaned Dataset\": diab_cleaned\n",
    "}\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analysis for {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split features and target\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    y = dataset.iloc[:, -1]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # (i) Train a Random Forest model\n",
    "    print(\"\\n--- Random Forest Model ---\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # (ii) Evaluate with classification report\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    print(\"\\nClassification Report for Random Forest:\")\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    \n",
    "    # (iii) Print feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importances for Random Forest:\")\n",
    "    print(feature_importances)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Random Forest Feature Importances - {dataset_name}')\n",
    "    plt.gca().invert_yaxis()  # Display the highest importance at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # (iv) Train XGBoost model\n",
    "    print(\"\\n--- XGBoost Model ---\")\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate XGBoost\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    print(\"\\nClassification Report for XGBoost:\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    # XGBoost feature importances\n",
    "    xgb_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importances for XGBoost:\")\n",
    "    print(xgb_importances)\n",
    "    \n",
    "    # Plot XGBoost feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(xgb_importances['Feature'], xgb_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'XGBoost Feature Importances - {dataset_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare the models\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    print(f\"Random Forest Accuracy: {rf_model.score(X_test, y_test):.4f}\")\n",
    "    print(f\"XGBoost Accuracy: {xgb_model.score(X_test, y_test):.4f}\")\n",
    "    \n",
    "    # Check if there are differences in the predictions\n",
    "    disagreements = np.sum(y_pred_rf != y_pred_xgb)\n",
    "    print(f\"Number of disagreements between models: {disagreements} out of {len(y_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7c935",
   "metadata": {},
   "source": [
    "### Create a SHAP summary plot that provides an overview of the average feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977fd8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a SHAP summary plot that provides an overview of the average feature importance\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# XGBoost model SHAP summary plot\n",
    "plt.subplot(1, 2, 1)\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)\n",
    "plt.title(\"XGBoost Feature Importance\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Random Forest model SHAP summary plot\n",
    "plt.subplot(1, 2, 2)\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "if isinstance(shap_values, list):  # For multi-class output\n",
    "    shap_values = shap_values[1]  # For binary classification, use class 1\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)\n",
    "plt.title(\"Random Forest Feature Importance\", fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff879eb",
   "metadata": {},
   "source": [
    "### Create SHAP waterfall plots that describe the model prediction for one or two individuals from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create SHAP waterfall plots that describe the model prediction for one or two individuals from the test dataset\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose two individuals from the test dataset\n",
    "individual_indices = [0, 5]  # You can change these indices if needed\n",
    "\n",
    "# Create a figure with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# For XGBoost Model\n",
    "explainer_xgb = shap.TreeExplainer(xgb_model)\n",
    "shap_values_xgb = explainer_xgb.shap_values(X_test)\n",
    "expected_value_xgb = explainer_xgb.expected_value\n",
    "\n",
    "# For Random Forest Model\n",
    "explainer_rf = shap.TreeExplainer(rf_model)\n",
    "shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "# Handle different return formats for Random Forest models\n",
    "if isinstance(shap_values_rf, list):  # For multi-class output\n",
    "    expected_value_rf = explainer_rf.expected_value[1]  # Class 1 expected value\n",
    "    shap_values_rf = shap_values_rf[1]  # Class 1 SHAP values\n",
    "else:\n",
    "    expected_value_rf = explainer_rf.expected_value\n",
    "\n",
    "# Create waterfall plots for each individual and model\n",
    "for i, idx in enumerate(individual_indices):\n",
    "    # XGBoost waterfall plot\n",
    "    plt.sca(axs[i, 0])\n",
    "    shap.waterfall_plot(shap.Explanation(\n",
    "        values=shap_values_xgb[idx], \n",
    "        base_values=expected_value_xgb, \n",
    "        data=X_test.iloc[idx], \n",
    "        feature_names=X_test.columns.tolist()\n",
    "    ), show=False)\n",
    "    plt.title(f\"XGBoost - Individual {idx}\", fontsize=14)\n",
    "    \n",
    "    # Random Forest waterfall plot\n",
    "    plt.sca(axs[i, 1])\n",
    "    shap.waterfall_plot(shap.Explanation(\n",
    "        values=shap_values_rf[idx], \n",
    "        base_values=expected_value_rf, \n",
    "        data=X_test.iloc[idx], \n",
    "        feature_names=X_test.columns.tolist()\n",
    "    ), show=False)\n",
    "    plt.title(f\"Random Forest - Individual {idx}\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual predictions for these individuals\n",
    "print(f\"Individual {individual_indices[0]} - XGBoost prediction: {xgb_model.predict_proba(X_test.iloc[[individual_indices[0]]])[:, 1][0]:.4f}, \"\n",
    "      f\"Actual: {y_test.iloc[individual_indices[0]]}\")\n",
    "print(f\"Individual {individual_indices[0]} - Random Forest prediction: {rf_model.predict_proba(X_test.iloc[[individual_indices[0]]])[:, 1][0]:.4f}, \"\n",
    "      f\"Actual: {y_test.iloc[individual_indices[0]]}\")\n",
    "print(f\"Individual {individual_indices[1]} - XGBoost prediction: {xgb_model.predict_proba(X_test.iloc[[individual_indices[1]]])[:, 1][0]:.4f}, \"\n",
    "      f\"Actual: {y_test.iloc[individual_indices[1]]}\")\n",
    "print(f\"Individual {individual_indices[1]} - Random Forest prediction: {rf_model.predict_proba(X_test.iloc[[individual_indices[1]]])[:, 1][0]:.4f}, \"\n",
    "      f\"Actual: {y_test.iloc[individual_indices[1]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pimaIndianData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.933279,
   "end_time": "2021-09-20T20:03:41.801824",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-20T20:02:51.868545",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
